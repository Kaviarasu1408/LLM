## Choosing right LLM

- open source or closed..
- release date or knowledge cut-off
- parameters
- Training Tokens
- Context length

- Inference cost(API Charge, Runtime compute)
- Training Cost
- Build cost
- Time to market
- Rate limits
- Speed
- Latencyy
- License

## Chinchilla scaling law

-Number of parameters that u need in a model is a roughly proportional to the size of your traning data
- If i want to train my model again, how many more parameters do i need the extra training data.
- the answer is we need to double the weight..initally if we have 8 billions, now its 16 billions parameters.
- it's an effective and powerful..
- if i say i from 8 billions, i want to go to 16 billions..then we need to double the training data